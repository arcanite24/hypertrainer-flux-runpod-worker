---
job: extension
config:
  name: "lora"
  process:
    - type: "sd_trainer"
      type: "sd_trainer"
      training_folder: "ai-toolkit/output"
      performance_log_every: 50
      device: cuda:0
      # trigger_word: ""
      network:
        type: "lora"
        linear: 8
        linear_alpha: 8
        dropout: 0.25
        network_kwargs:
          only_if_contains:
            - transformer.single_transformer_blocks.7.proj_out
            - transformer.single_transformer_blocks.20.proj_out
      save:
        dtype: float16
        save_every: 5
        max_step_saves_to_keep: 2
      datasets:
        - folder_path: "ai-toolkit/dataset"
          caption_ext: "txt"
          caption_dropout_rate: 0.05
          shuffle_tokens: false
          cache_latents_to_disk: true
          resolution: [512]
      train:
        batch_size: 32
        steps: 10
        gradient_accumulation_steps: 1
        train_unet: true
        train_text_encoder: false
        gradient_checkpointing: true
        noise_scheduler: "flowmatch"
        optimizer: "prodigy"
        optimizer_params:
          weight_decay: 0.01
          decouple: true
          d0: 0.0001
          use_bias_correction: false
          safeguard_warmup: false
          d_coef: 1.0
        lr: 1
        skip_first_sample: true

        ema_config:
          use_ema: true
          ema_decay: 0.99

        dtype: bf16
      model:
        name_or_path: "black-forest-labs/FLUX.1-dev"
        is_flux: true
        quantize: true
      sample:
        sampler: "flowmatch"
        sample_every: 100
        width: 1024
        height: 1024
        prompts:
          - a corgi wearing a party hat and sunglasses
        neg: ""
        seed: 42
        walk_seed: false
        guidance_scale: 3.5
        sample_steps: 20
meta:
  name: "[name]"
  version: "1.0"
