job: extension
config:
  name: "lora"
  process:
  - type: sd_trainer
    training_folder: "output"
    performance_log_every: 25
    device: cuda:0
    trigger_word: ""
    network:
      type: lora
      linear: 8
      linear_alpha: 8
      dropout: 0.25
      network_kwargs:
        only_if_contains:
        - transformer.single_transformer_blocks.7.proj_out
        - transformer.single_transformer_blocks.20.proj_out
    save:
      dtype: float16
      save_every: 50
      max_step_saves_to_keep: 2
      save_format: safetensors
    datasets:
    - folder_path: "dataset"
      caption_ext: txt
      caption_dropout_rate: 0
      token_dropout_rate: 0
      cache_latents_to_disk: true
      resolution:
      - 512
    train:
      batch_size: 32
      loss_type: mse
      train_unet: true
      train_text_encoder: false
      steps: 100
      gradient_checkpointing: true
      noise_scheduler: flowmatch
      skip_first_sample: true
      ema_config:
        use_ema: true
        ema_decay: 0.9999
      dtype: bf16
      lr: 1
      optimizer: prodigy
      optimizer_params:
        weight_decay: 0.01
        decouple: true
        d0: 0.0001
        use_bias_correction: false
        safeguard_warmup: false
        d_coef: 1.0
    model:
      name_or_path: black-forest-labs/FLUX.1-dev
      is_flux: true
      quantize: true
    sample:
      sampler: flowmatch
      sample_every: 100
      width: 1024
      height: 1024
      prompts:
      - a corgi wearing sunglases and a party hat
      neg: ''
      seed: 42
      walk_seed: false
      guidance_scale: 3.5
      sample_steps: 25
meta:
  name: [name]
  version: '1.0'